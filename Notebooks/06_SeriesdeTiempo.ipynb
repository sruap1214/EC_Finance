{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7afeb4e-a725-4733-ba56-0629374a123b",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>Modelación Financiera I</b></h1>\n",
    "<h1 align=\"center\"><b> Módulo 6 </b></h1>\n",
    "<h1 align=\"center\"><b> Series de tiempo  </b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3cb9a-0cfa-4612-a196-948c2002448a",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "***Docente:*** Santiago Rúa Pérez, PhD.\n",
    "\n",
    "***e-mail:*** srua@udemedellin.edu.co\n",
    "\n",
    "***Herramienta:*** [Jupyter Notebook](http://jupyter.org/)\n",
    "\n",
    "***Kernel:*** Python 3.7\n",
    "\n",
    "***MEDELLÍN - COLOMBIA***\n",
    "\n",
    "***2022***\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841377aa-60e8-4831-9921-fa961104eeb1",
   "metadata": {},
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#Trabajando-con-series-de-tiempo\" data-toc-modified-id=\"Trabajando-con-series-de-tiempo-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Trabajando con series de tiempo</a></span></li>\n",
    "    <li><span><a href=\"#Fechas-y-tiempos-nativos-de-Python\" data-toc-modified-id=\"Fechas-y-tiempos-nativos-de-Python-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Fechas y tiempos nativos de Python</a></span></li>\n",
    "    <li><span><a href=\"#Estructura-de-series-de-tiempo-con-Pandas\" data-toc-modified-id=\"Estructura-de-series-de-tiempo-con-Pandas-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Estructura de series de tiempo con Pandas</a></span></li>\n",
    "    <li><span><a href=\"#Remuestreo,-desplazamiento-y-creaci%C3%B3n-de-ventanas\" data-toc-modified-id=\"Remuestreo,-desplazamiento-y-creaci%C3%B3n-de-ventanas-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Remuestreo, desplazamiento y creación de ventanas</a></span></li>\n",
    "    <li><span><a href=\"#Ventanas-m%C3%B3viles\" data-toc-modified-id=\"Ventanas-m%C3%B3viles-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Ventanas móviles</a></span></li>\n",
    "    <li><span><a href=\"#Descomponer-series-de-tiempo\" data-toc-modified-id=\"Descomponer-series-de-tiempo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Descomponer series de tiempo</a></span></li>\n",
    "    <li><span><a href=\"#Usar-y-remover-tendencias\" data-toc-modified-id=\"Usar-y-remover-tendencias-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Usar y remover tendencias</a></span></li>\n",
    "    <li><span><a href=\"#Usar-y-remover-estacionalidad\" data-toc-modified-id=\"Usar-y-remover-estacionalidad-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Usar y remover estacionalidad</a></span></li>\n",
    "    <li><span><a href=\"#Estacionaridad-en-series-de-tiempo\" data-toc-modified-id=\"Estacionaridad-en-series-de-tiempo-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Estacionaridad en series de tiempo</a></span></li>\n",
    "    <li><span><a href=\"#Algunos-conceptos-antes-de-hacer-pron%C3%B3sticos\" data-toc-modified-id=\"Algunos-conceptos-antes-de-hacer-pron%C3%B3sticos-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Algunos conceptos antes de hacer pronósticos</a></span></li>\n",
    "    <li><span><a href=\"#M%C3%A9todos-de-Box-Jenkins\" data-toc-modified-id=\"M%C3%A9todos-de-Box-Jenkins-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Métodos de Box Jenkins</a></span></li>\n",
    "    <li><span><a href=\"#Laboratorio\" data-toc-modified-id=\"Laboratorio-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Laboratorio</a></span></li>\n",
    "    </ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44465862-ed74-41ec-b7d8-d64c3e0d8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394aa19-671a-4137-b585-29b17724de17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trabajando con series de tiempo\n",
    "\n",
    "Pandas fue desarrollado en el contexto de modelación financiera, por lo anterior tiene un conjunto extenso de herramientas para trabajar con fechas, tiempo, y datos indexados con tiempo. Fechas y datos de tiempo vienen con las siguientes características:\n",
    "\n",
    "- Las etiquetas de tiempo referencia momentos particulares, ejemplo: July 4th, 2015 at 7:00am\n",
    "- Periodos e intervalos de tiempo hacen referencia a longitudes que tienen inicio y final, por ejemplo: año 2015\n",
    "- Deltas de tiempo hacen referencia a longitud exacta, ejemplo: 22.5 segundos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b224b-ea44-4b48-b08d-8716f8b9a3d2",
   "metadata": {},
   "source": [
    "## Fechas y tiempos nativos de Python\n",
    "\n",
    "El objeto o clase por defecto para trabajar con fechas en python es el modulo interno de `datetime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8342e53c-72fb-43d1-885c-979a9fec0ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 7, 4, 0, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d3c6f2-d568-4859-a0ac-9986f109761f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 7, 4, 0, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5a42d7-f068-42d6-b388-6e7faebd7ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4d00d-3e62-4710-8396-77ed9297e2b5",
   "metadata": {},
   "source": [
    "En la última línea, se uso el formato estándar de salida para imprimir fechas, el cual puede leer más información en [strftime section](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).\n",
    "\n",
    "La potencia de estas librerias está en su fácil uso y flexibilidad en su sintaxis. Donde fallan es cuando se requiere trabajar con listado grande de fechas y tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2c414-b82b-40ee-8425-228a4e29fbb4",
   "metadata": {},
   "source": [
    "### Fechas y tiempo en Pandas\n",
    "\n",
    "Pandas esta construido sobre estas herramientas y crea un objeto propio llamado `Timestamp`, el cual combina las librerias antes vistas con el manejo eficiente vectorizado. Pandas entonces es capaz de construir in indice tipo `DatetimeIndex` el cual puede ser usado tanto en series como en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04254c4c-db2b-42c5-a080-51070d255f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-07-25 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "date = pd.to_datetime(\"25th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db249bc7-01cb-4195-9c11-97ef182b40ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359259ca-5819-42f3-a44b-341a7d88f32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-25', '2015-07-26', '2015-07-27', '2015-07-28',\n",
       "               '2015-07-29', '2015-07-30', '2015-07-31', '2015-08-01',\n",
       "               '2015-08-02', '2015-08-03', '2015-08-04', '2015-08-05'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4637f4-18d5-44ec-96b5-1f0c2607adb1",
   "metadata": {},
   "source": [
    "Entonces, como podemos realizar indexación de un objeto tipo serie o dataframe teniendo en cuenta el tiempo?\n",
    "\n",
    "El primer paso es crear el objeto tipo `DatetimeIndex` y crear dicha serie o dataframe con el index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a698f99-3934-42cb-9bc7-f1f563bdf4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-07-04    0\n",
       "2014-08-04    1\n",
       "2015-07-04    2\n",
       "2015-08-04    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34beeda-a105-4551-846b-820e67b0b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-07-04    0\n",
       "2014-08-04    1\n",
       "2015-07-04    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['2014-07-04':'2015-07-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853ae4a4-e130-4b49-818f-c0a8e8e0f8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-07-04    2\n",
       "2015-08-04    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c62e0-1153-45aa-9d1a-5fe3f1171049",
   "metadata": {},
   "source": [
    "## Estructura de series de tiempo con Pandas\n",
    "\n",
    "A continuación se hablará de las estructuras de datos fundamentales de Pandas para trabajar con series de tiempo\n",
    "\n",
    "- Para *time stamps*, Pandas introduce el tipo `Timestamp`. Es un remplazo de la libreria nativa de Python `datetime`, pero mas eficiente. La estructura para el indice está dada por `DatetimeIndex`\n",
    "- Para *time Periods*, Pandas introduce el tipo `Period`. Este codifica un intervalo fijo y cuya estructura para el indice está dado por `PeriodIndex`.\n",
    "- Para *time deltas* o *durations*, Pandas introduce el tipo `Timedelta`. Este es una forma más eficiente de la libreria nativa `datetime.timedelta` y su estructura de indice está dado por `TimedeltaIndex`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394dfd7f-c82c-4cd7-9d65-249e02a1739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',\n",
       "               '2015-07-08'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n",
    "                       '2015-Jul-6', '07-07-2015', '20150708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564cbde2-b4da-4891-b4be-43c69a6d3fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',\n",
       "             '2015-07-08'],\n",
       "            dtype='period[D]')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598738f1-01d1-42c0-bc61-51d8b39ab9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb3ff9-b2d8-47c1-b77f-defee3b5854e",
   "metadata": {},
   "source": [
    "Para la creación de estas fechas, Pandas nos ofrece un conjunto de funciones los cuales pueden ser usados de la siguiente forma: `pd.date_range()` para timestamps, `pd.period_range()` para periodos, y `pd.timedelta_range()` para deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a27f963-0def-4fc7-938c-7d0ad7ea1350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',\n",
       "               '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2cb9298-89f2-49d2-989f-a64ec5ba89d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',\n",
       "               '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.date_range('2015-07-03', periods=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bbfca11-3d9b-44ae-87eb-30d52f01c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00',\n",
       "               '2015-07-03 02:00:00', '2015-07-03 03:00:00',\n",
       "               '2015-07-03 04:00:00', '2015-07-03 05:00:00',\n",
       "               '2015-07-03 06:00:00', '2015-07-03 07:00:00'],\n",
       "              dtype='datetime64[ns]', freq='H')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.date_range('2015-07-03', periods=8, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5625997f-e8dd-420d-9124-3ab31635b5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12',\n",
       "             '2016-01', '2016-02'],\n",
       "            dtype='period[M]')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.period_range('2015-07', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67bd6c7b-50c5-49cf-9591-b2949067caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00',\n",
       "                '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00',\n",
       "                '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00',\n",
       "                '0 days 09:00:00'],\n",
       "               dtype='timedelta64[ns]', freq='H')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c611ccd-9722-4b32-a89d-30f0d2416b7a",
   "metadata": {},
   "source": [
    "Es importante entender la notación para los códigos de los calendarios\n",
    "\n",
    "| Code   | Description         | Code   | Description          |\n",
    "|--------|---------------------|--------|----------------------|\n",
    "| ``D``  | Calendar day        | ``B``  | Business day         |\n",
    "| ``W``  | Weekly              |        |                      |\n",
    "| ``M``  | Month end           | ``BM`` | Business month end   |\n",
    "| ``Q``  | Quarter end         | ``BQ`` | Business quarter end |\n",
    "| ``A``  | Year end            | ``BA`` | Business year end    |\n",
    "| ``H``  | Hours               | ``BH`` | Business hours       |\n",
    "| ``T``  | Minutes             |        |                      |\n",
    "| ``S``  | Seconds             |        |                      |\n",
    "| ``L``  | Milliseonds         |        |                      |\n",
    "| ``U``  | Microseconds        |        |                      |\n",
    "| ``N``  | nanoseconds         |        |                      |\n",
    "\n",
    "\n",
    "| Code    | Description            || Code    | Description            |\n",
    "|---------|------------------------||---------|------------------------|\n",
    "| ``MS``  | Month start            ||``BMS``  | Business month start   |\n",
    "| ``QS``  | Quarter start          ||``BQS``  | Business quarter start |\n",
    "| ``AS``  | Year start             ||``BAS``  | Business year start    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b811a77-9745-41cc-92fd-9e91b7fdbfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00',\n",
       "                '0 days 07:30:00', '0 days 10:00:00', '0 days 12:30:00',\n",
       "                '0 days 15:00:00', '0 days 17:30:00', '0 days 20:00:00'],\n",
       "               dtype='timedelta64[ns]', freq='150T')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e945074-4209-4b3b-8e45-b308affbf1af",
   "metadata": {},
   "source": [
    "## Remuestreo, desplazamiento y creación de ventanas\n",
    "\n",
    "La capacidad de usar fechas y horas como índices para organizar y acceder de manera intuitiva a los datos es una pieza importante de las herramientas de series temporales de Pandas. Los beneficios de los datos indexados en general (alineación automática durante las operaciones, división y acceso intuitivo de datos, etc.) aún se aplican, y Pandas proporciona varias operaciones adicionales específicas de series temporales.\n",
    "\n",
    "Para ver esto, se usará algunos datos de precios de acciones como ejemplo. Debido a que Pandas se desarrolló principalmente en un contexto financiero, incluye algunas herramientas muy específicas para datos financieros. Por ejemplo, el paquete `pandas-datareader` que lo acompaña (instalable a través de `conda install pandas-datareader`), sabe cómo importar datos financieros de varias fuentes disponibles, incluidas Yahoo Finance, Quandl, Tiingo y otras. Aquí cargaremos el historial de precios de cierre de Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e03dcbd-e8fb-47ae-b063-9c07e5d29a3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "download() got an unexpected keyword argument 'start_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39myfinance\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39myf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# goog = data.DataReader('GOOG', start='2004', end='2016',\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#                        data_source='yahoo')\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m goog \u001b[39m=\u001b[39m  yf\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39mGOOGL\u001b[39;49m\u001b[39m'\u001b[39;49m, start_date \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m2004\u001b[39;49m\u001b[39m'\u001b[39;49m, end_date \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m2016\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m goog\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mTypeError\u001b[0m: download() got an unexpected keyword argument 'start_date'"
     ]
    }
   ],
   "source": [
    "# from pandas_datareader import data\n",
    "import yfinance as yf\n",
    "\n",
    "# goog = data.DataReader('GOOG', start='2004', end='2016',\n",
    "#                        data_source='yahoo')\n",
    "goog =  yf.download('GOOGL', start='2004', end='2016')\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11395c52-d2d6-43e9-bf8a-6cdfbba891e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = goog['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6893b3a-0278-489d-83ff-106392adef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246c4c1-f12e-41b3-9a12-351ecce2b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "goog.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd960834-3d8c-495e-a063-71c3ea532972",
   "metadata": {},
   "source": [
    "Una necesidad común de datos de series temporales es volver a muestrear a una frecuencia más alta o más baja. Esto se puede hacer usando el método `resample()`, o el método mucho más simple `asfreq()`. La principal diferencia entre los dos es que `resample()` es fundamentalmente una agregación de datos, mientras que `asfreq()` es fundamentalmente una selección de datos.\n",
    "\n",
    "Echando un vistazo al precio de cierre de Google:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea75d35-9108-4c73-abbd-0c72599e177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog.plot(alpha=0.5, style='-')\n",
    "goog.resample('BA').mean().plot(style=':')\n",
    "goog.asfreq('BA').plot(style='--');\n",
    "plt.legend(['input', 'resample', 'asfreq'],\n",
    "           loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa52a9-40b2-473d-8ac0-ba891e8337d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = goog.resample('BA').mean()\n",
    "print(a)\n",
    "b = goog.asfreq('BA')\n",
    "print(b)\n",
    "print(goog.loc['2004-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d2564-ddc6-4a1b-9495-b9d7bf1e5a56",
   "metadata": {},
   "source": [
    "Otra operación común específica de series de tiempo es el desplazamiento de datos en el tiempo. Pandas tiene dos métodos estrechamente relacionados para calcular esto: `shift()` y `tshift()`. En resumen, la diferencia entre ellos es que `shift()` cambia los datos, mientras que `tshift()` cambia el índice. En ambos casos, el cambio se especifica en múltiplos de la frecuencia.\n",
    "\n",
    "Aquí haremos `shift()` y `tshift()` por 900 días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc900aa-1a33-4faf-b3ca-87f6edfdff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3)\n",
    "\n",
    "# apply a frequency to the data\n",
    "goog = goog.asfreq('D', method='pad')\n",
    "\n",
    "goog.plot(ax=ax[0])\n",
    "goog.shift(900).plot(ax=ax[1])\n",
    "goog.tshift(900).plot(ax=ax[2])\n",
    "\n",
    "# legends and annotations\n",
    "local_max = pd.to_datetime('2007-11-05')\n",
    "offset = pd.Timedelta(900, 'D')\n",
    "\n",
    "ax[0].legend(['input'], loc=2)\n",
    "ax[0].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[0].axvline(local_max, alpha=0.3, color='red')\n",
    "\n",
    "ax[1].legend(['shift(900)'], loc=2)\n",
    "ax[1].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[1].axvline(local_max + offset, alpha=0.3, color='red')\n",
    "\n",
    "ax[2].legend(['tshift(900)'], loc=2)\n",
    "ax[2].get_xticklabels()[1].set(weight='heavy', color='red')\n",
    "ax[2].axvline(local_max + offset, alpha=0.3, color='red');\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.1, \n",
    "                    hspace=0.6)\n",
    "\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a97bb-1424-4329-b350-0219a1ac291d",
   "metadata": {},
   "source": [
    "Tipicamente este tipo de operaciones se utiliza para realizar diferencia o diferenciación en el tiempo. Por ejemplo, se puede usar los valores desplazados para calcular el retorno de la inversión de la acción de Google en un año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d544819-963e-43b9-a2b8-15ea164d90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = 100 * (goog.shift(-365) / goog - 1)\n",
    "ROI.plot()\n",
    "plt.ylabel('% retorno de la inversión');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71db15-2752-4d08-863c-a8d345baa10a",
   "metadata": {},
   "source": [
    "## Ventanas móviles\n",
    "\n",
    "Pandas posibilita realizar implementaciones de ventanas moviles a series temporales en sus datos. Esto se puede lograr a través del atributo `rolling()` de los objetos `Series` y `DataFrame`, que devuelve una vista similar a la que vimos con la operación groupby. Esta vista continua pone a disposición una serie de operaciones de agregación de forma predeterminada.\n",
    "\n",
    "Por ejemplo, aquí está la media móvil centrada en un año y la desviación estándar de los precios de las acciones de Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61957c-0279-42a8-b6af-dc9c6be81ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = goog.rolling(365, center=True)\n",
    "\n",
    "data = pd.DataFrame({'input': goog,\n",
    "                     'one-year rolling_mean': rolling.mean(),\n",
    "                     'one-year rolling_std': rolling.std()})\n",
    "ax = data.plot(style=['-', '--', ':'])\n",
    "ax.lines[0].set_alpha(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972f716-3ed2-42dc-ae6b-e0aa7a504c54",
   "metadata": {},
   "source": [
    "Si desea conocer mas sobre este tema, consulte la página directa de Pandas [\"Time Series/Date\" section](http://pandas.pydata.org/pandas-docs/stable/timeseries.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de8ee3-0863-413e-ae9d-6e47a6c349ae",
   "metadata": {},
   "source": [
    "## Descomponer series de tiempo\n",
    "\n",
    "La descomposición de series de tiempo implica pensar en una serie como una combinación de nivel (level), tendencia (trend), estacionalidad (seasonality), y componentes de ruido (noise). La descomposición proporciona un modelo abstracto útil para pensar en las series de tiempo de forma general y para comprender mejor los problemas durante el análisis y pronóstico de series de tiempo.\n",
    "\n",
    "- **Nivel**: el promedio de los valores en la serie\n",
    "- **Tendencia**: el incremento o decremento en la serie\n",
    "- **Estacionalidad**: el ciclo de repetición de ciclo corto en la serie\n",
    "- **Ruido**: variaciones aleatorias\n",
    "\n",
    "Existen dos formas de caracterizar la señal, como un modelo aditivo o como un modelo multiplicativo\n",
    "\n",
    "Para analizar estas caracteristicas, utilizaremos la libreria `statsmodel` y usaremos de nuevo la acción de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9b9e4-ba5f-4064-8b93-8244f212f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "\n",
    "goog = data.DataReader('GOOG', start='2004', end='2016',\n",
    "                       data_source='yahoo')\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fa713-4f6a-40a2-9688-fc02a96b4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "# seasonla decompose need freq\n",
    "CloseStockGoogle = goog['Close'].asfreq('D', method='pad')\n",
    "result = seasonal_decompose(CloseStockGoogle, model='additive')\n",
    "\n",
    "fig, ax = plt.subplots(4)\n",
    "\n",
    "result.observed.plot(ax=ax[0])\n",
    "result.seasonal.plot(ax=ax[1])\n",
    "result.trend.plot(ax=ax[2])\n",
    "result.resid.plot(ax=ax[3])\n",
    "\n",
    "ax[0].set_ylabel(\"Observed\")\n",
    "\n",
    "ax[1].set_ylabel(\"Seasonal\")\n",
    "\n",
    "ax[2].set_ylabel(\"Trend\")\n",
    "\n",
    "ax[3].set_ylabel(\"Resid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd68e9-c58e-445f-b6f0-2a5185c88609",
   "metadata": {},
   "source": [
    "Se puede observar de esa gráfica que la tendencia del cierre de la acción es de crecimiento, mientras que no presenta estacionalidad. Vamos a generar una señal aleatoria para mirar todos los comportamientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948d82d-493a-47b7-a86e-9d8eeb42adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplicative decompose time series\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "series = read_csv('./data/airline-passengers.csv', \n",
    "                  header = 0, \n",
    "                  index_col = 0, \n",
    "                  parse_dates = True).squeeze(\"columns\")\n",
    "result = seasonal_decompose(series, model='multiplicativ')\n",
    "result.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063b99e-c968-4e63-9367-f0d6feb23eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.rc(\"figure\", figsize=(16, 12))\n",
    "plt.rc(\"font\", size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb1fc0-38b0-4a0c-b504-55432b167648",
   "metadata": {},
   "source": [
    "## Usar y remover tendencias\n",
    "\n",
    "Las tendencias no son mas que un incremento o decremento continuo en nuestra series de tiempo. A veces es importante analizarlas o inclusive removerlas de la serie. Desde una definición formal: es un cambio sistemático en la serie de tiempo que no aparece de forma periódica. \n",
    "\n",
    "En la práctica identificar una tendencia puede ser dificil ya que en ocasiones puede ser subjetivo. Existen dos formas de remover tendencias: utilizando diferenciación o encontrando un modelo y restarle dicho modelo. \n",
    "\n",
    "Solo se analizará utilizando diferenciación. El dataset a utilizar describe el numero de ventas de shampoo sobre un periodo de 3 años. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d3e13-88fb-44a5-a6e9-1329d32517a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend a time series using differencing\n",
    "import pandas as pd\n",
    "\n",
    "initYear = 2019\n",
    "\n",
    "def parser(x):\n",
    "    year, month = x.split(\"-\")\n",
    "    year = str(int(year) + initYear - 1)\n",
    "    return pd.datetime.strptime(year + \"-\" + month, '%Y-%m')\n",
    "\n",
    "series = pd.read_csv('./data/shampoo_sales.csv', \n",
    "                     header=0, \n",
    "                     index_col=0, \n",
    "                     parse_dates=True, \n",
    "                     date_parser=parser).squeeze(\"columns\")\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536fcd3-2409-4871-963e-c64787022b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crea figura de tamaño 6,6 inches\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(121)     # Subplot de 1 fila, 2 columnas, Primera columna\n",
    "plt.plot(series.diff())\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "\n",
    "ax1 = fig.add_subplot(122)     # Subplot de 1 fila, 2 columnas, Primera columna\n",
    "plt.plot(series)\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff68f8-87a2-4b39-8ccf-d3065ae46b52",
   "metadata": {},
   "source": [
    "## Usar y remover estacionalidad\n",
    "\n",
    "En el caso contrario, a veces es necesario analizar las tendencias y no la estacionalidad de las series de tiempo. Los ciclos repetitivos pueden esconder información relevante del proceso. Formalmente se define como: un patron repetitivo que se repite cada x periodo de tiempo.\n",
    "\n",
    "Una serie de tiempo con claro componente de estacionalidad es conocida como no-estacionaria. Remover esta característica dentro de las serie de tiempo se puede utilizar los dos mismo métodos vistos anteriormente. Esta vez usaremos otro dataset que contiene la temperatura diaria minímia en una zona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57336d-70c9-4e27-b2ed-1bb330ee1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "series = pd.read_csv('./data/daily-minimum-temperatures.csv',\n",
    "                    header = 0,\n",
    "                    index_col = 0,\n",
    "                    parse_dates = True).squeeze(\"columns\")\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d6fd8-7313-4fd2-959b-80ce75db8eba",
   "metadata": {},
   "source": [
    "Antes de analizar que tipo de estacionalidad tiene la serie de tiempo, es importante que miremos como se comporta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177766b-ede0-4ba1-b277-68aef12cb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524591f8-1e9c-41b7-8a26-2d92a0c82494",
   "metadata": {},
   "source": [
    "Notese que la grafica parece que tuviera una estacionalidad de mas o menos cada año. Podemos realizar la diferenciación año a año. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9d699-2e68-4317-9bfb-8b970a8ecd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(series.diff(periods = 365))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30447d46-b3c9-402e-b982-7dd3eb8aa547",
   "metadata": {},
   "source": [
    "Otra opción que se puede considerar es que la temperatura en un periodo de tiempo fijo se mantengan igual. Por ejemplo, asumamos que la temperatura deberia ser la misma mes a mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf2132-7782-4e36-9770-6b251f7e3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample = series.resample('M')\n",
    "monthly_mean = resample.mean()\n",
    "print(monthly_mean.head())\n",
    "plt.plot(monthly_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657aa3fd-49f7-41dc-a139-4f992bad3bc0",
   "metadata": {},
   "source": [
    "Mediante este método vemos claramente la estacionalidad de la serie de tiempo que se da mes a mes. Ahora si a esta gráfica tratamos de remover la estacionalidad, entonces tenderemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e2a50-3957-4057-8693-128d0766cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monthly_mean.diff(periods=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ab580-49c9-493f-a90d-f32b58cc0c8d",
   "metadata": {},
   "source": [
    "## Estacionaridad en series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4118c-09be-4aff-8c6e-f9eb61f4f6cc",
   "metadata": {},
   "source": [
    "Las observaciones en una serie de tiempo estacionaria no son dependientes del tiempo. Lo anterior significa que si obtengo la características estadísticas tales como el promedio o la varianza, serán la misma para cualquier instante de tiempo. Cuando las series son estacionarias, son fáciles de modelar. Generalmente los modelos estadísticos asumen esta condición para ser modelados. \n",
    "\n",
    "Una serie de tiempo que muestre efectos estacionales, tendencias u otras estructuras dependiente del tiempo son no estacionarias. El analisis clásico consiste en convertir estas series no estacionarias en estacionarias quitando tendencias, o efecto de estacionalidad. Los métodos para clasificar si es estacionaria o no, se pueden dividir en tres:\n",
    "\n",
    "- **Gráficas**: se puede realizar un análisis de las gráficas en el tiempo de los datos y visualmente chequear la existencia de tendencia o estacionalidad\n",
    "- **Estadística descriptiva**: se puede realizar un analisis de las variables estadísticas de la señala realizando particiones aleatorias y chequear diferencias significativas\n",
    "- **Test estadísticos**: chequear si las espectativas de los test se cumplen\n",
    "\n",
    "\n",
    "Vamos a realizar el analisis de dos datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127055b9-e075-462e-8eb4-4e388455b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "series1 = pd.read_csv('./data/daily-total-female-births.csv', \n",
    "                     header = 0, \n",
    "                     index_col = 0, \n",
    "                     parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "series2 = pd.read_csv('./data/airline-passengers.csv', \n",
    "                     header = 0, \n",
    "                     index_col = 0, \n",
    "                     parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "ax1 = fig.add_subplot(221)     # Subplot de 2 fila, 2 columnas, Primera columna\n",
    "plt.plot(series1)\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.title('Female Births')\n",
    "\n",
    "ax2 = fig.add_subplot(222)     # Subplot de 1 fila, 2 columnas, Primera columna\n",
    "plt.plot(series2)\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.title('Arilines passengers')\n",
    "\n",
    "ax3 = fig.add_subplot(223)     # Subplot de 1 fila, 2 columnas, Primera columna\n",
    "plt.hist(series1)\n",
    "\n",
    "ax4 = fig.add_subplot(224)     # Subplot de 1 fila, 2 columnas, Primera columna\n",
    "plt.hist(series2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4673d-993a-434c-8391-70dff985416c",
   "metadata": {},
   "source": [
    "Notese que estamos buscando la media y la varianza de las señales, por lo que asumimos que la respuesta o histograma deberia ser de forma gaussiana. Notese que la gráfica de la izquierda tiene un comportamiento gaussiano con un poco de cola a la derecha. Sin embargo, la gráfica de la derecha no se ve para nada guassiana. Este tipo de gráfica puede ser indicación de una serie de tiempo no estacionaria. Intentemos dividir la secuencia en dos tramas y calcular la estadística descriptiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccaf8a-f0a3-40e8-94e5-849529d2f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = int(len(series1) / 2)\n",
    "series1_X1, series1_X2 = series1[0:split1], series1[split1:]\n",
    "series1_mean1, series1_mean2 = series1_X1.mean(), series1_X2.mean()\n",
    "series1_var1, series1_var2 = series1_X1.var(), series1_X2.var()\n",
    "print('Series 1 -- mean1 = %9.2f, mean2 = %9.2f' % (series1_mean1, series1_mean2))\n",
    "print('Series 1 -- variance1 = %.2f, variance2 = %.2f' % (series1_var1, series1_var2))\n",
    "\n",
    "split2 = int(len(series2) / 2)\n",
    "series2_X1, series2_X2 = series2[0:split2], series1[split2:]\n",
    "series2_mean1, series2_mean2 = series2_X1.mean(), series2_X2.mean()\n",
    "series2_var1, series2_var2 = series2_X1.var(), series2_X2.var()\n",
    "print('Series 2 -- mean1 = %9.2f, mean2 = %9.2f' % (series2_mean1, series2_mean2))\n",
    "print('Series 2 -- variance1 = %.2f, variance2 = %.2f' % (series2_var1, series2_var2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786f313-a477-4953-89c5-92ba182a4024",
   "metadata": {},
   "source": [
    "Recuerde que la gráfica de pasajeros tiene claramente una componente de estacionalidad y que ademas tiene un comportamiento de crecimiento en el tiempo. Vamos a realizar una operación sobre esta serie de tiempo y ver que pasa con la estadística descriptiva de dicha señal. Apliquemos el logaritmo y volvamos a realizar el mismo analisis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f943b7-4f40-47bf-becd-a44253b77f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.log(series2)\n",
    "split = int(len(X) / 2)\n",
    "X1, X2 = X[0:split], X[split:]\n",
    "mean1, mean2 = X1.mean(), X2.mean()\n",
    "var1, var2 = X1.var(), X2.var()\n",
    "print('mean1=%.2f, mean2=%.2f' % (mean1, mean2))\n",
    "print('variance1=%.2f, variance2=%.2f' % (var1, var2))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "plt.plot(X)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.hist(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d66687-aef2-4a90-9d4d-e0aa8fc875aa",
   "metadata": {},
   "source": [
    "Notese que este método puede ser fácilmente engañado, es mejor usar test estadísticos, tales como los de **Dickey-Fuller** o **Kwiatkowski-Phillips-Schmidt-Shin**. Statsmodels nos presenta una serie de test los cuales podemos aplicar a nuestra serie de tiempo. El objetivo de este test es aceptar la hipotesis nula (es decir, la serie de tiempo no es estacionaria) o rechazarla (aceptar la alternativa, lo que signfica que es estacionaria). Para interpretar estos resultados usaremos el valor-p.\n",
    "\n",
    "- **p-value $>$ 0.05** se falla en rechazar la hipotesis nula, es decir, la serie de tiempo es no estacionaria\n",
    "- **p-value $\\leq$ 0.05** rechazar la hipotesis nula, es decir, la serie de tiempo es estacionaria\n",
    "\n",
    "Hagamos el analisis para las tres series que vimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf244155-6f78-44c0-b981-8012ef8777a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result_births = adfuller(series1)\n",
    "result_passenger = adfuller(series2)\n",
    "result_logPassenger = adfuller(np.log(series2))\n",
    "\n",
    "\n",
    "print('ADF Statistic for birth dataset: %f' % result_births[0])\n",
    "print('p-value: %f' % result_births[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result_births[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    \n",
    "print('ADF Statistic for passenger dataset: %f' % result_passenger[0])\n",
    "print('p-value: %f' % result_passenger[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result_passenger[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    \n",
    "print('ADF Statistic for log passenger dataset: %f' % result_logPassenger[0])\n",
    "print('p-value: %f' % result_logPassenger[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result_logPassenger[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19455584-b682-4286-a453-1817d0bd57d8",
   "metadata": {},
   "source": [
    "Notese que para el primer dataset, el valor es muy inferior al umbral por lo que se rechaza la hipotesis nula, es decir, es estacionaria. Las otras dos series de tiempo son no estacionarias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3082370-dedb-4266-9a78-5e9c3a2fe23b",
   "metadata": {},
   "source": [
    "## Algunos conceptos antes de hacer pronósticos\n",
    "\n",
    "Para saber que tan bueno es un modelo o no, necesitamos tener claro una metodologia para hacerlo, sobretodo si se trata de modelación basado en datos. Adicionalmente, asumamos que un modelo se comporta bien, no se podrá deteriorar en el tiempo?\n",
    "\n",
    "Cuando tratamos de ajustar un modelo a los datos, no solo es importante cuantificar como se comporta con esos datos, sino también como se comporta ante datos que no ha visto anteriormente. Cuando trabajamos en series de tiempo, la evaluación de estos modelos se llama **backtesting**. PAra realizar este procedimiento es importante:\n",
    "\n",
    "- Dividir el conjunto de datos en Entreamiento-Testeo que respeten el orden temporal de las observaciones\n",
    "- Realizar en lo posible múltiples divisiones que respeten el orden temporal\n",
    "- Validación hacia adelante, en donde el modelo se pudiera actualizar cada vez que recibe nuevos datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ff0de-ca1a-4cbd-8604-e0622fff596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "series1 = pd.read_csv('./data/daily-total-female-births.csv', \n",
    "                     header = 0, \n",
    "                     index_col = 0, \n",
    "                     parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "train_size = int(len(series1) * 0.80)\n",
    "train, test = series1[0:train_size], series1[train_size:len(series1)]\n",
    "print('Observations: %d' % (len(series1)))\n",
    "print('Training Observations: %d' % (len(train)))\n",
    "print('Testing Observations: %d' % (len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad1912-ef40-492b-a3db-f42d089350e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "plt.plot(series1,'k')\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.plot(train)\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82323e-7c40-4400-9051-68d054c6eb18",
   "metadata": {},
   "source": [
    "Finalmente es importante saber como cuantificar el error del modelo con respecto a los datos obtenidos. Las tres formas más usadas son:\n",
    "\n",
    "- **Error absoluto medio (MAE)**: es calculado como el promedio del valor absoluto de los errores en cada punto\n",
    "- **Error cuadrático medio (MSE)**: es calculado como el promedio de los errores elevados al cuadrado\n",
    "- **Raiz cuadrada del error cuadrático medio (RMSE)**: la raiz cuadrada del MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f664ce-4125-4f5c-abc4-6c0d689ef26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "expected = [0.0, 0.5, 0.0, 0.5, 0.0]\n",
    "predictions = [0.2, 0.4, 0.1, 0.6, 0.2]\n",
    "\n",
    "MAE = np.mean(np.abs(np.array(expected)-np.array(predictions)))\n",
    "MSE = np.mean((np.array(expected)-np.array(predictions))**2)\n",
    "RMSE = np.sqrt(np.mean((np.array(expected)-np.array(predictions))**2))\n",
    "\n",
    "print('MAE: %7.4f' % MAE)\n",
    "print('MSE: %7.4f' % MSE)\n",
    "print('RMSE: %5.4f' % RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea6397-989c-466b-94bf-70577b55b4c6",
   "metadata": {},
   "source": [
    "Veremos más adelante que la libreria para machine learning nos presentan estas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19cd37-92d7-4141-abf2-ad28a33e77a6",
   "metadata": {},
   "source": [
    "## Métodos de Box Jenkins\n",
    "\n",
    "El objetivo de esta metodologia es la de encontrar el mejor modelo para hacer pronóstico en series de tiempo basado en modelos autoregresivos de media movil (**ARMA**) si es estacionario o modelos autoregresivos integrados de media movil (**ARIMA**) si no son estacionarios. Cada componente de estos modelos esta definida como:\n",
    "\n",
    "- **AR**: autoregresión. Es aquel modelo que usa un relación dependiente entre la observación actual y alguna cantidad de observaciones retrasadas.\n",
    "- **I**: integración. Es aquel modelo que usa la diferenciación entre las observaciones con el objetivo de volverla en estacionaria\n",
    "- **MA**: media móvil: Es aquel modelo que usa la dependencia entre las diferentes observaciones y los errores residuales de un modelo medio móvil aplicado a variables retrasadas. \n",
    "\n",
    "Estas tres componentes estan definidas explicitamente en el modelo **ARIMA(p,d,q)**\n",
    "- **p**: el número de observaciones retrasadas incluidas en el modelo\n",
    "- **d**: el número de veces que las observaciones son diferenciadas\n",
    "- **q**: el tamaño de la ventana media móvil. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b93be-9bb1-4a42-b708-a00e86de4c6a",
   "metadata": {},
   "source": [
    "### Autocorrelación\n",
    "\n",
    "Ya que los modeos de autoregresión asumen que la observación siguiente dependen de cierta forma de las anteriores, entonces es importante analizar la correlación de la señal. Si las dos señales cambian en la misma dirección, entonces es una correlación positiva, sino sería negativa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f80de-32f3-437e-8a96-05d38c143c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "series_temperatura = pd.read_csv('./data/daily-minimum-temperatures.csv',\n",
    "                                header = 0,\n",
    "                                index_col = 0,\n",
    "                                parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(series_temperatura[0:-1],series_temperatura[1:])\n",
    "plt.xlabel('y(t)')\n",
    "plt.ylabel('y(t+1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b8964-72df-43e1-bbb7-992774008b03",
   "metadata": {},
   "source": [
    "Notese que se muestra una correlación entre ambas variables. Un test estadísticos que podría usar es el coeficiente de **correlación de Pearson** el cual indica +1 correlacionado positivamente y -1 correlacionado negativamente. Valores muy cercanos a cero significan baja correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88692f-31d5-4dc4-858e-30e6765ebd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame()\n",
    "max_lagg = 5\n",
    "for i in range(max_lagg):\n",
    "    column_name = 't+' + str(i)\n",
    "    dataframe[column_name] = series_temperatura.shift(max_lagg-1-i)\n",
    "\n",
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00737fd1-0697-4f4d-9a42-de70c4433714",
   "metadata": {},
   "source": [
    "En aras de hacer mas eficiente este procedimiento, Pandas crea un función que gráfica la autocorrelación de la señal el cual incluye un intervalo de confianza del 95% y 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f84cb-bda9-488e-92fc-cfcbae2ca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(series_temperatura)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ad28d-00b9-49c6-af11-d7de385ff2f9",
   "metadata": {},
   "source": [
    "Statsmodels también nos incluye un método para realizar este tipo de gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aae54c-2bee-49f6-bf4b-86614cd2642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(series_temperatura, lags=35);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d87a4-c550-471b-bfcf-35d2fcff42d7",
   "metadata": {},
   "source": [
    "### Modelo de autoregresión (AR)\n",
    "\n",
    "Recuerde que un modelo de autoregresion AR($p$) esta definido como\n",
    "\n",
    "$$x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + ... + \\alpha_p x_{t-p} + w_t$$\n",
    "\n",
    "Tratemos de obtener nuestro primer modelo de autoregresion. Antes de eso es importante tener un modelo de linea base para comparar los resultados. El modeo más simple que se puede trabajar, es decir, que la temperatura siguiente es igual a la del día anterior. Implementemos esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b24197-ae3d-492d-b314-502617353537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "series = pd.read_csv('./data/daily-minimum-temperatures.csv',\n",
    "                    header = 0, \n",
    "                    index_col = 0,\n",
    "                    parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "# create lagged dataset\n",
    "dataframe = pd.concat([series.shift(1), series], axis=1)\n",
    "dataframe.columns = ['t', 't+1']\n",
    "\n",
    "# split into train and test sets\n",
    "X = dataframe.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]\n",
    "\n",
    "# persistence model\n",
    "def model_persistence(x):\n",
    "    return x\n",
    "\n",
    "# walk-forward validation\n",
    "predictions = list()\n",
    "for x in test_X:\n",
    "    yhat = model_persistence(x)\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "    \n",
    "rmse = np.sqrt(np.mean((np.array(test_y)-np.array(predictions))**2))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# plot predictions vs expected\n",
    "plt.plot(test_y)\n",
    "plt.plot(predictions, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d49bcf-660d-47f0-9d0c-dff36f64ccba",
   "metadata": {},
   "source": [
    "Ahora intentemos encontrar un modelo de autoregresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2b12c-73c2-4446-bf29-7dc0f4e86fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# load dataset\n",
    "series = pd.read_csv('./data/daily-minimum-temperatures.csv',\n",
    "                    header = 0, \n",
    "                    index_col = 0,\n",
    "                    parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "# split dataset\n",
    "X = series.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "\n",
    "# train autoregression\n",
    "model = AutoReg(train, lags = 29)\n",
    "model_fit = model.fit()\n",
    "print('Coefficients: %s' % model_fit.params)\n",
    "\n",
    "\n",
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "\n",
    "rmse = np.sqrt(np.mean((np.array(test_y)-np.array(predictions))**2))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# plot predictions vs expected\n",
    "plt.plot(test_y)\n",
    "plt.plot(predictions, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c994c-fe58-4361-a4b0-9166cdd7d7e6",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "a continuación miraremos como es la clase u objeto para realizar estimaciones de modelo usando ARIMA. Para lo anterior utilizaremos el dataset de ventas de shampoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188445c0-4a7a-4735-986f-eea29d86d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# load dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "series = pd.read_csv('./data/shampoo_sales.csv', \n",
    "                     header=0, \n",
    "                     index_col=0, \n",
    "                     parse_dates=True, \n",
    "                     date_parser=parser).squeeze(\"columns\")\n",
    "\n",
    "plt.plot(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b698b8-dc69-47c3-8fd0-08238ebdf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.read_csv('./data/shampoo_sales.csv', \n",
    "                     header=0, \n",
    "                     index_col=0, \n",
    "                     parse_dates=True,\n",
    "                     date_parser=parser).squeeze(\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f4649-f0e4-419c-a55d-3853169eab12",
   "metadata": {},
   "source": [
    "Notese que la serie de tiempo tiene una tendencia, por lo que sugiere que no es estacionaria y habrá que aplicar diferenciación. Miremos la gráfica de autocorrelación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fbc03-fc6f-4cac-88ce-f3e5ca1ae77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf23054-4510-4beb-9b79-e6000b639b2e",
   "metadata": {},
   "source": [
    "Notese que hay una correlación significativa positva de los primeros 5 lags. Eso quiere decir que un bueno inicio para AR puede ser de 5. Teniendo en cuenta esto, y sabiendo que una buena opción es realizar al menos un primer grado de diferenciación por la tendencia procedamos a modelar usando ARIMA\n",
    "\n",
    "Recuerde que los pasos para modelar son los siguientes:\n",
    "\n",
    "- Cargue el dataset\n",
    "- Divida su dataset en conjunto de entrenamiento y test\n",
    "- Creo una instancia del objeto a trabajar, en este caso ARIMA()\n",
    "- Llame al método fit para hacer el ajuste.\n",
    "- Lllame al método predict para hacer las predicciones\n",
    "- Obtengan alguna métrica de su modelo sobre los datos de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e138199-c251-45d5-a529-53959dc62476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# load dataset\n",
    "series = pd.read_csv('./data/shampoo_sales.csv',\n",
    "                    header = 0, \n",
    "                    index_col = 0,\n",
    "                    parse_dates = True).squeeze(\"columns\")\n",
    "# split dataset\n",
    "X = series.values\n",
    "size = int(len(X) * 0.70)\n",
    "train, test = X[1:size], X[size:]\n",
    "train_history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(train_history, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()[0]\n",
    "    predictions.append(output)\n",
    "    train_history.append(test[t])\n",
    "\n",
    "#print(model_fit.summary())\n",
    "\n",
    "# Evaluation metric\n",
    "rmse = np.sqrt(np.mean((np.array(test)-np.array(predictions))**2))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# plot predictions vs expected\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.legend(['Test','Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37fbb6-0244-4ab6-b5f0-1e09dff4d03f",
   "metadata": {},
   "source": [
    "Notese que el modelo todavia puede ser ajustado algunos parámetros adicionales. Cómo podriamos hacer ajuste de hiperparámetros? Uno de los métodos mas utiizados es mediante un busqueda en grilla, en donde se evaluan diferentes parametros ajustables del modelo. Es importante entonces, tener claridad en la métrica a evaluar para diferenciar cual modelo se comporta mejor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50138ddc-d565-4a8f-a606-b1a30c809c7d",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Como se mencionó anteriormente, el objetivo con este método es evaluar los diferentes parámetros de un modelo para saber cual es la mejor opción. En nuestro caso usaremos la raiz cuadrática del error cuadrático medio, y se realizará un un walk-forward cada vez que se tenga un dato. Primero creemos la función que evalue el modelo de ARIMA. Para realizar esta función, tendremos en cuenta los parámetros de recepción, en este caso, el dataset de entrenamiento y el orden del modelo de ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332d2cb-4983-42b1-abd2-bfd7b13352ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(endog = history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "        \n",
    "    # calculate out-of-sample error\n",
    "    rmse = np.sqrt(np.mean((np.array(test)-np.array(predictions))**2))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873a011-2bf5-41fd-a7a4-bae43117e906",
   "metadata": {},
   "source": [
    "Con esta función en mente, ya solo nos queda crear una función que recorra un conjunto de parámetros y nos evalue el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6919ba-b088-4f53-98ee-b512cb2a9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5be791-218a-4226-bc79-b5fd20b436cd",
   "metadata": {},
   "source": [
    "Ahora con estas dos funciones, podremos evaluar los modelos y encontrar la mejor combinación de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde6fcd-0cb0-49b5-911b-736d7710a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# load dataset\n",
    "series = pd.read_csv('./data/shampoo_sales.csv',\n",
    "                    header = 0, \n",
    "                    index_col = 0,\n",
    "                    parse_dates = True).squeeze(\"columns\")\n",
    "\n",
    "# Ignore warnings in training\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# split dataset\n",
    "X = series.values\n",
    "size = int(len(X) * 0.90)\n",
    "train, test = X[1:size], X[size:]\n",
    "\n",
    "# evaluate parameters\n",
    "p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "evaluate_models(train, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e546383-34d4-4328-981a-9978cbf14ff2",
   "metadata": {},
   "source": [
    "Conociendo la mejor combinación de parámetros, se procede a analizar como se comporta con respecto al test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699f647-d594-42db-8bd1-ba6085a79931",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_order = (4, 1, 2)\n",
    "predictions = list()\n",
    "history = [x for x in train]\n",
    "\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(endog = history, order = arima_order)\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.forecast()[0]\n",
    "    predictions.append(yhat)\n",
    "    history.append(test[t])\n",
    "    \n",
    "# Evaluation metric\n",
    "rmse = np.sqrt(np.mean((np.array(test)-np.array(predictions))**2))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# plot predictions vs expected\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.legend(['Test','Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52fed6-d72c-4b7d-b37f-e4edf4d4ed38",
   "metadata": {},
   "source": [
    "Finalmente para guardar el modelo, basta con utilizar el método `save` y guardarlo con extensión pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a7a62-42ac-4750-b414-9c87f35f5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "model_fit.save('./data/ARIMA_ShampooDataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03703e-0a2d-4f81-a8cf-2c985a7c17ca",
   "metadata": {},
   "source": [
    "### Finalizando ...\n",
    "\n",
    "Entiendo un poco como funciona esta libreria y como se realiza el llamado a los diferentes métodos, ya puede explorar otras opciones tales como:\n",
    "   \n",
    "- Otros test estadísticos como KPSS, u otros modelos tales como SARIMAX (Seasonal Autoregressive Integrated Moving Average with eXogenous regressor model)\n",
    "- También podrían explorar otras librerias que implementen estos modelos tales como darts.\n",
    "- Realizar analisis de volatilidad, usando la libreria arch https://arch.readthedocs.io/en/latest/univariate/introduction.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20789e32-5067-4552-bbdf-0d8cdc49afe5",
   "metadata": {},
   "source": [
    "## Laboratorio\n",
    "\n",
    "- ***Ejercicio 1.*** Consiga una base de datos correspondiente a los temas que usted trabaja a diario. Con lo visto en clase realice visualizaciones de la misma y responda preguntas propias en donde implique realizar operaciones de agrupación, agregaciones, entre otras. \n",
    "\n",
    "\n",
    "- ***Ejercicio 2.*** Proceda a descargarse el conjunto de datos temporales sobre el uso o conteo de bicicletas en el [Puente Fremont] de Seattle (http://www.openstreetmap.org/#map=17/47.64813/-122.34965). Estos datos provienen de un contador de bicicletas automatizado, instalado a fines de 2012, que tiene sensores inductivos en las aceras este y oeste del puente.\n",
    "\n",
    "    Los recuentos de bicicletas por hora se pueden descargar de [link](https://data.seattle.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k).\n",
    "    \n",
    "    Cargue el archivo a un dataframe y obtenga información util tal como: la distribución del tráfico de bicicletas en el día, o como se compara este con respecto a los fines de semana. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfinance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "89cafe07fa9106b7e0014ff380c907a9e86547fc22d76eb24752761b967d32b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
